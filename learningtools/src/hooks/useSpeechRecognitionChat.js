import { useState, useRef, useEffect, useCallback } from 'react';
import { getTranslation } from '../utils/translations';

export const useSpeechRecognition = (selectedLanguage, onResult) => {
  const [isListening, setIsListening] = useState(false);
  const [transcript, setTranscript] = useState('');
  const recognitionRef = useRef(null);
  const isInitializedRef = useRef(false);

  // ğŸ”’ CRÃTICO: Acumular TODOS os resultados finais
  const finalTranscriptRef = useRef('');

  // ğŸ†• NOVO: Rastrear o Ãºltimo Ã­ndice processado
  const lastProcessedIndexRef = useRef(0);

  // ğŸš€ FLAG DE INTENÃ‡ÃƒO: Controla se DEVE enviar ao parar
  const shouldAutoSendRef = useRef(true);

  useEffect(() => {
    // âœ… Evitar inicializaÃ§Ã£o dupla
    if (isInitializedRef.current) return;

    const SpeechRecognition = window.SpeechRecognition || window.webkitSpeechRecognition;

    if (!SpeechRecognition) {
      console.error('âŒ Speech Recognition nÃ£o suportado neste navegador');
      return;
    }

    isInitializedRef.current = true;
    const newRecognition = new SpeechRecognition();

    newRecognition.lang = selectedLanguage;
    newRecognition.continuous = true;
    newRecognition.interimResults = true;
    newRecognition.maxAlternatives = 1;

    newRecognition.onstart = () => {
      console.log('âœ… Recognition started');
      setIsListening(true);
      setTranscript('ğŸ¤ Listening...');
      shouldAutoSendRef.current = true;
      lastProcessedIndexRef.current = 0; // ğŸ†• Reset do Ã­ndice
    };

    newRecognition.onresult = (event) => {
      let interimTranscript = '';

      // ğŸ†• FIX: Processar apenas NOVOS resultados a partir do Ãºltimo Ã­ndice
      for (let i = lastProcessedIndexRef.current; i < event.results.length; i++) {
        const transcriptPart = event.results[i][0].transcript;

        if (event.results[i].isFinal) {
          // âœ… ACUMULAR resultados finais APENAS UMA VEZ
          console.log('ğŸ“ Final result added:', transcriptPart);
          finalTranscriptRef.current += transcriptPart + ' ';
          lastProcessedIndexRef.current = i + 1; // ğŸ†• Atualizar Ã­ndice processado
        } else {
          interimTranscript += transcriptPart;
        }
      }

      // âœ… Mostrar: texto acumulado + preview interim
      setTranscript(
        (finalTranscriptRef.current.trim() + ' ' + interimTranscript).trim()
      );
    };

    newRecognition.onerror = (event) => {
      console.error('âŒ Speech Recognition Error:', event.error);
      const t = getTranslation(selectedLanguage);
      setIsListening(false);

      if (event.error === 'no-speech') {
        onResult('', t.noSpeech);
      } else if (event.error === 'not-allowed') {
        onResult('', 'ğŸ”’ Microphone permission denied');
      } else if (event.error !== 'aborted') {
        onResult('', 'Error: ' + event.error);
      }
    };

    newRecognition.onend = () => {
      console.log('ğŸ›‘ Recognition ended, shouldAutoSend:', shouldAutoSendRef.current);
      setIsListening(false);

      const textToSend = finalTranscriptRef.current.trim();

      // ğŸ”’ CRÃTICO: SÃ“ ENVIA SE shouldAutoSendRef = true
      if (textToSend && shouldAutoSendRef.current) {
        console.log('ğŸ“¤ Auto-sending:', textToSend);
        setTranscript(textToSend);
        onResult(textToSend, '');
      }

      // Reset para prÃ³xima sessÃ£o
      finalTranscriptRef.current = '';
      lastProcessedIndexRef.current = 0; // ğŸ†• Reset do Ã­ndice
      shouldAutoSendRef.current = true;
    };

    recognitionRef.current = newRecognition;
    console.log('âœ… Speech Recognition initialized');

    return () => {
      if (recognitionRef.current) {
        recognitionRef.current.abort();
      }
    };
  }, []); // âœ… DEPENDÃŠNCIA VAZIA - inicializa uma vez sÃ³

  // âœ… Atualizar idioma sem reinicializar
  useEffect(() => {
    if (recognitionRef.current && !isListening) {
      recognitionRef.current.lang = selectedLanguage;
    }
  }, [selectedLanguage, isListening]);

  const toggleListening = useCallback((action = 'toggle') => {
    console.log('ğŸ¤ toggleListening called with action:', action);

    if (!recognitionRef.current) {
      console.error('âŒ Recognition not initialized');
      const t = getTranslation(selectedLanguage);
      onResult('', t.notSupported);
      return;
    }

    if (isListening) {
      // ğŸ”’ Se clicou em X (cancelar), nÃ£o envia
      if (action === 'cancel') {
        console.log('âŒ Cancel: shouldAutoSend = false');
        shouldAutoSendRef.current = false;
      }
      // Se clicou em âœ… (enviar manual), envia
      else if (action === 'send') {
        console.log('âœ… Send: shouldAutoSend = true');
        shouldAutoSendRef.current = true;
      }

      recognitionRef.current.stop();
      return;
    }

    // âœ… INICIAR GRAVAÃ‡ÃƒO
    finalTranscriptRef.current = '';
    lastProcessedIndexRef.current = 0; // ğŸ†• Reset do Ã­ndice
    shouldAutoSendRef.current = true;
    setTranscript('');

    try {
      console.log('ğŸ¤ Starting recognition...');
      recognitionRef.current.start();
    } catch (error) {
      console.error('âŒ Error starting recognition:', error);
      setIsListening(false);
    }
  }, [isListening, selectedLanguage, onResult]);

  return { isListening, transcript, setTranscript, toggleListening };
};